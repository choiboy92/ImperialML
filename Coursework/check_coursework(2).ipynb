{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "check_coursework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIaqPNWwYn-c"
      },
      "source": [
        "This notebook is for checking ME4 Machine Learning coursework submission. You should run your model through this to check that you are getting the right answer prior to submission. \n",
        "If you get the wrong answers out, this is an indication that you need to change your model, not an indication that this script is incorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBmTOrSGY6UL"
      },
      "source": [
        "The section below is just used for clearing any previously uploaded files. You should not normally need to run it, unless you have had crashes. If you run it you will need to re-upload the datasets. You can always open the file manager on the left hand side to look at the files too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73fIVRGcU490"
      },
      "source": [
        "#clear all files from the work area - only really need to do this if uploading\n",
        "#files multiple times\n",
        "!ls -lh\n",
        "!rm *.csv\n",
        "!rm *.h5\n",
        "!rm *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tubtFG9sZLBu"
      },
      "source": [
        "This section is for uploading the two training datasets. You can do these together if you wish by selecting both in the upload window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doDknfj9Qn6B"
      },
      "source": [
        "#upload training data sets here (run this once)\n",
        "from google.colab import files\n",
        "\n",
        "print('Upload CSV dataset files here')\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR1sP882ZYZf"
      },
      "source": [
        "---\n",
        "This section uploads the files selected (note -- select both .h5 and .txt for scaling parameters if you have them, otherwise it will not do any scaling), and runs the training data you have uploaded through the model. You should check that this all works OK -- you don't get any errors -- and that the fraction correct matches what you saw when you trained it. Note that this only works with one model at a time so you will need to run it twice -- once each for dataset 1 and 2. \n",
        "\n",
        "Also note that if you upload files with the same name multiple times then they will be renamed to e.g. huthwaite-peter-2 (1).h5 which will cause issues - the script tries to catch this by deleting any uploaded files at the end, but if it crashes you may need to delete all uploaded files using the section at the top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK8IyjA0PsgJ"
      },
      "source": [
        "#upload model files\n",
        "import numpy as np\n",
        "import os.path\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from google.colab import files\n",
        "print('Note: please process one model at a time - this tool is not designed to do both')\n",
        "print('Upload one .h5 file and corresponding scaling file (if using one) here')\n",
        "uploaded = files.upload()\n",
        "h5file = ''\n",
        "scalefile = ''\n",
        "scale_exists = False\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  filename, file_extension = os.path.splitext(fn)\n",
        "  if file_extension == '.h5':\n",
        "    h5file = fn\n",
        "  if file_extension == '.txt':\n",
        "    scalefile = fn\n",
        "    scale_exists = True\n",
        "  #print(\"Ext: \", file_extension)\n",
        "  \n",
        "  filebase = filename\n",
        "  \n",
        "#print('File base is: ', filebase)\n",
        "print('H5 file is: ', h5file)\n",
        "print('Scale file is: ', scalefile)\n",
        "\n",
        "#don't do this on Colab\n",
        "# #user defined parameters (change these as necessary):\n",
        "# #put your name here:\n",
        "# givenname = 'Peter'\n",
        "# familyname = 'Huthwaite'\n",
        "# #set which dataset to use:\n",
        "# dataset = 1\n",
        "\n",
        "#filebase = familyname.lower()+'-'+givenname.lower()+'-'+np.str(dataset)\n",
        "\n",
        "dataset = filebase[-1]\n",
        "#print('Dataset considered is: ',dataset)\n",
        "\n",
        "#load in student model\n",
        "model = load_model(h5file,compile=False)\n",
        "\n",
        "if scale_exists:\n",
        "  if os.path.exists(scalefile):\n",
        "      print(scalefile+' exists - loading in scaling parameters')\n",
        "      scaleArray = np.loadtxt(scalefile)\n",
        "  else:\n",
        "      print(scalefile+' not found - assuming no scaling')\n",
        "      scaleArray = np.array([np.zeros([6,]), np.ones([6,])])\n",
        "else:\n",
        "    print('No scale file provided - assuming no scaling')\n",
        "    scaleArray = np.array([np.zeros([6,]), np.ones([6,])])\n",
        "\n",
        "#load in the data provided to the students\n",
        "df = pd.read_csv('dataset' + np.str(dataset) + '.csv')\n",
        "\n",
        "Lt = np.array(df['Arm length (m)'][:])\n",
        "Wt = np.array(df['Ball weight (kg)'][:])\n",
        "Rt = np.array(df['Ball radius (mm)'][:])\n",
        "Tt = np.array(df['Air temperature (deg C)'][:])\n",
        "Et = np.array(df['Spring constant (N per m)'][:])\n",
        "Dt = np.array(df['Device weight (kg)'][:])\n",
        "Ot = np.array(df['Target hit'][:])\n",
        "XtUnscaled = np.column_stack([Lt, Wt, Rt, Tt, Et, Dt])\n",
        "\n",
        "# use values to scale validation data in XvUnscaled (whose shape is [number_of_validations,6])\n",
        "Xt = (XtUnscaled-scaleArray[0,:])/scaleArray[1,:]\n",
        "\n",
        "Yt = to_categorical(Ot)\n",
        "#run the data through the model\n",
        "Yt_predict = model.predict(Xt)\n",
        "\n",
        "#output a summary of the model if you wish\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "number_correct = 0\n",
        "for i in range(len(Yt)):\n",
        "    if np.round(Yt[i, 0]) == np.round(Yt_predict[i, 0]):\n",
        "        number_correct += 1\n",
        "\n",
        "fraction_correct = 1.0 * number_correct / len(Yt_predict)\n",
        "\n",
        "#import to make it bold\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "printmd(\"**Fraction correct with training data is: \"+np.str(fraction_correct)+\"**\")\n",
        "\n",
        "if fraction_correct < 0.6:\n",
        "    printmd('**Warning: very poor performance on training data; likely error**')\n",
        "\n",
        "#print('Tidying up - removing h5 file and scale file if it exists')\n",
        "os.remove(h5file)\n",
        "if scale_exists:\n",
        "  os.remove(scalefile)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}